{"name":"Labrador","tagline":"A dead simple crawler","body":"### Welcome to Labrador.\r\nLabrador is a free (MIT license), dead simple crawler based on Crawler4j.\r\n(Actually it's a fetcher with configurations you can easily manipulate.\r\n\r\n### How to use?\r\nConfigure and run (no programming required)\r\n\r\n### How to configure?\r\nConfigurations are stored in directory ```data```:\r\n+ ```data/config/crawler.properties```: misc configurations\r\n+ ```data/config/domains.txt```: line-by-line domains to crawl\r\n+ ```data/config/exclude.txt```: regular expressions to exclude Urls \r\n+ ```data/config/seeds.txt```: several urls to initialize crawling processes\r\n\r\n### How to run?\r\nYou can find Labrador binary in directory ```dist```.\r\nTo run it, simply issue commands:\r\n```shell \r\njava -jar labrador-assembly-1.0.jar\r\n```\r\nor\r\n```shell \r\nnohup java -jar labrador-assembly-1.0.jar  > runtime.log &\r\n```\r\n### How to read the output?\r\nThe output will be stored in directory ```data/storage``` (each domain has its own directory)\r\n\r\nFor example: all pages fetched from ```http://vnexpress.net``` will be stored in ```data/storage/vnexpress.net```\r\n\r\nTo get Html pages downloaded for a specific domain, you must know that:\r\n\r\n+ In the domain directory, there's a file named ```site.info``` storing downloaded urls in separated lines\r\n\r\n+ Lines' indices will be used to name files storing associated pages\r\n \r\nBelow is the pseudo code to extract downloaded content:\r\n\r\n```python\r\ndomain_directory = 'data/storage/vnexpress.net/'\r\ninfo = read_file(domain_directory + 'site.info')\r\nindex = 0\r\nfor url in info.get_lines():\r\n    content = read_file(domain_directory + index.to_string())\r\n    print \"url = %s with content = %s\" % (url, content)\r\n```\r\n\r\n### How to rebuild Labrador?\r\nLabrador is written in Scala. To rebuild, please use SBT:\r\n\r\n```shell\r\nsbt assembly\r\n```\r\n\r\n### Author\r\nAnh Le @ bigsonata.com","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}